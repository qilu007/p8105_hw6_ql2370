---
title: "HW6"
author: "QiLu"
date: "11/22/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(modelr)

```

## Problem 1
```{r}
child_data = read.csv(file = "./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  ) %>% 
  drop_na()
 
  
#child_data = map(child_data, as.factor) %>% 
#  as.tibble()
```
  
  Fit a linear model 
```{r}
bwt_full = lm(bwt ~ ., data = child_data)
summary(bwt_full)
```

Let's look at the p-value, at the 0.05 significant level, babysex, bhead, blength, delwt,gaweeks, mrace, parity and smoken are fail to reject the null hypothesis that coefficients are zero. However, an ANOVA test can optimize this model. I decided to test mom's race and parity which have higher p-value compared to others.

```{r, warning=FALSE}
# ANOVA test
bwt_lm = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mrace + parity + smoken, data = child_data)
bwt_test = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + smoken, data = child_data)

anova(bwt_full, bwt_test) %>% 
  broom::tidy()

```
Therefore, we reject the null hypothesis. mom's race and parity can be included as predictors. I use bwt_lm as my linear model as the following
```{r}
summary(bwt_lm)
```
Showing the plot of model residuals against fitted values
```{r}

child_data %>% 
  modelr::add_residuals(bwt_lm) %>% 
  ggplot(aes(x = bwt, y = resid)) + geom_violin()

```


```{r}
cv_df = 
  crossv_mc(child_data, 50) 

cv_df %>% pull(train) %>% .[[3]] %>% as_tibble

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(linear_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mrace + parity + smoken, data = child_data)),
         main_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = child_data)),
         inter_mod = map(train, ~lm(bwt ~ bhead * blength * babysex, data = child_data)),
          rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(.x, .y)),
    rmse_main = map2_dbl(.x = main_mod, .y = test, ~rmse(.x, .y)),
    rmse_inter = map2_dbl(inter_mod, test, ~rmse(model = .x, data = .y)))
```


```{r}
cv_result = cv_df %>% 
  select(rmse_linear, rmse_main, rmse_inter) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  as_tibble()


  ggplot(data = cv_result,aes(x = model, y = rmse)) + geom_violin()
```



## Problem 2
```{r}
# read in data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
bootstp_lm = lm(tmax ~ tmin, data = weather_df)
r_sq = broom::glance(bootstp_lm)[,1]

```

```{r}
plot_r = weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance)) %>% 
  select(results) %>% 
  unnest(results) 
```

```{r}
ggplot(data = plot_r, aes(x = r.squared)) + geom_density()
```

```{r}
plot_b = weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(results) %>% 
  unnest(results) 

even_idx = seq(2,5000,2)
odd_idx = seq(1,5000,2)

b0_set = data.frame(b0 = plot_b$estimate[odd_idx])
b1_set = data.frame(b1 = plot_b$estimate[even_idx])
est_set = log(b0_set * b1_set )

ggplot(data = est_set, aes(x =b0)) +
  geom_density() +
  labs(x = "Log(b0 * b1)")
```

  
The plots above show that the distributions of log(b0 times b1) seems like normal after bootstraping samples to analyze the association between tmax and min. R square after bootstraping mainly lays on 0.90 to 0.92, which means the model is real good for statstical standard, but this distribution has a heavy tail extending to low values and a bit of a “shoulder”, features that may be related to the frequency with which large outliers are included in the bootstrap sample.

```{r}
r_CI =  quantile(plot_r$r.squared, probs=c(0.025, 0.975), na.rm=TRUE)
log_b_CI = quantile(est_set$b0, probs=c(0.025, 0.975), na.rm=TRUE)
```


```{r}
r_CI
log_b_CI
```

